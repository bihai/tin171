\documentclass[a4paper,11pt]{article}
\usepackage{fancyhdr}
\setlength{\headheight}{11pt}
\pagestyle{fancyplain}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{graphicx}
%\renewcommand{\chaptermark}[1]{\markboth{#1}{}}

\lhead{ }
\rhead{}
%\renewcommand{\headrulewidth}{0.0pt}

\lfoot{Group 1: final report}
\cfoot{\thepage}
\rfoot{}

%
%%    homebrew commands -- to save typing
\newcommand\etc{\textsl{etc}}
\newcommand\eg{\textsl{eg.}\ }
\newcommand\etal{\textsl{et al.}}
\newcommand\Quote[1]{\lq\textsl{#1}\rq}
\newcommand\fr[2]{{\textstyle\frac{#1}{#2}}}
\newcommand\miktex{\textsl{MikTeX}}
\newcommand\comp{\textsl{The Companion}}
\newcommand\nss{\textsl{Not so Short}}


\begin{document}
%-----------------------------------------------------------
\title{Final report\\Fun With Chinese Checkers}
\author{Group 1: Emil Falk, Rodolphe Lepigre,\\
        Salvatore Tomaselli, G\"oran Weinholt}
\maketitle
%-----------------------------------------------------------
\begin{abstract}%\centering
%%COMMENT (re-use from project proposal and modify if needed)
Chinese Checkers is a board game for up to six players. The fact that
it is a multi-player game makes it quite different from more widely
explored two-player games like Chess. In writing a computer player for
Chinese Checkers we have explored search algorithms and heuristics.
Our aim is to highlight the ways in which multi-player games differ
from two-player games, and the consequences this has for the
effectiveness of a computer player.
\end{abstract}

\section{Introduction and Background}
% (A short general text about the area of ai that your project belongs to.)
Chinese Checkers is a board game that can be played by two to six players.
Each player controls ten pegs that are placed in the corners of a
hexagonal star. The board has little holes in it where the pegs can be
placed. In each round of the game a player must move a peg in one of
two ways. A peg can be moved to one of the six holes next to it or, if
one of the holes is occupied by another peg, it can also ``jump'' over
that peg in a straight line (thereby traveling two holes over). The
player can jump over other pegs an unlimited number of times every
round, potentially moving a peg straight across the whole board. The
objective is to move all pegs to the opposite corner.
% Maybe it would be nice to add a few lines about the history of the game ?

\subsection{Problem statement}
% (Re-use from project proposal, add changes, refinements,
% extensions. If there has been major changes since the project
% proposal, describe and motivate.)
We have implemented a computer program capable of playing Chinese
Checkers. Our goal has been to create a program that could beat a
novice human player.

\subsection{Related work}
% (Re-use form project proposal, extend with furhter findings from
% literature study, cite and add references. Expain which are relevant
% for your project and which not and why.)
Sturtevant has explored the max$^n$ and paranoid algorithms in
relation to Chinese Checkers
\cite{springerlink:10.1007/978-3-540-40031-8_8}. The max$^n$ algorithm
is a generalisation of minimax to $n$-player games. The paranoid
algorithm uses the idea that all other players have formed a coalition
(which means there are only really two players). Later on he also
evaluated the UCT algorithm
\cite{springerlink:10.1007/978-3-540-87608-3_4}, which is described as
``Monte-Carlo-like'' and quite effective against max$^n$, but
requiring a lot of computation time.

An interesting point that Sturtevant makes is that multi-player games
(i.e.~Chinese Checkers) are difficult for computers for two reasons:
search strategies are less effective than for two-player games and
there is a need for opponent modelling which is normally not required.

Huang describes a contest in which he supervised students who wrote
programs to play Chinese Checkers \cite{Huang:2001:SGP:378593.378708}.
The students used iter\-ative-deep\-ening search and experimented with
heuristics. We believe that this will be a good way of starting our
project.

Hashavit and Markovitch evaluated the Max-Prob algorithm
\cite{Hashavit}. The algorithm is similar to max$^n$ and paranoid, but
in Chinese Checkers it is significantly better. It computes at each
step which move will be most likely to lead to a winning outcome. It
does this for all players and maximizes its own probability of
winning.

Ulfhake reports on a program for two-player Chinese Checkers
\cite{ulfhake}. The program has a variety of search algorithms
available, including different variants of alphabeta and minimax
search. Something particularly interesting to us is her description of
the heuristics she used. The program was reported to play excellently
against human players.

%% Research into programs that play games is likely to be geared towards
%% producing strong programs that one day will be able to beat the best
%% human players. Perhaps a weaker program would be just as suitable for
%% our needs.

\subsection{Tools and programs available}
% (Re-use and extend your list of tools and programs from the project
% proposal including references.
% Which ones are you actually using?  Cite and annotate them.
% Did you encounter any practical difficulties?)
There is a free software implementation of Chinese Checkers called
\emph{cheech}. It provides a graphical interface and a network
protocol. It initially appeared to us that we could make our own
program connect to \emph{cheech} as a player and thereby use the
existing multi-player functionality and graphics. Unfortunately the program has
not been updated for years and uses an old version of the GTK toolkit wrapper
for C++ and other libraries, and some of the functions it was using have been
removed or replacing. So it fails to build from source and after some failed
effort to adapt it to the new libraries that are shipped on modern systems we
decided to abandon it.
We evaluated half a dozen alternatives and found that none of them
still work properly. That being said, we did copy the server-client
model of \emph{cheech} for our own work.

\section{Our solution}
% (Re-use the content and structure of subsections from project
% proposal. Add refinments and extensions, motivate changes compared
% to earlier stages of the project.)
\subsection{Significance}

An interesting aspect of Chinese Checkers is that it is a multi-player
game. This means that players can form temporary coalitions against
other players. While traditional two-player strategies can be used to
play the game they are not going to be able to play it perfectly and
in this sense Chinese Checkers is still an open problem. Working on an
open problem is always interesting.

The multi-player attribute provides a good setting for objectively
evaluating the performance of different algorithms and heuristics. The
existing literature is somewhat sparse on how existing computer
programs fare against humans in multi-player Chinese Checkers. We hope
to rectify that with this report.

\subsection{The central idea}

The architecture of our solution consists of three parts: a game
server written in Erlang, a graphical game client written in Python
and several autonomous game clients. A network protocol has been
specified that supports multiple running games. The server coordinates
the game and tells each player what is happening to the board and
prompts players to make their move. The server verifies that the moves
are legal and checks if there is a winner. The graphical client can
connect either as a player or a spectator. It shows a graphical
representation of the board and lets the user make moves.

%% Initially our program will be playing a two-player variant of Chinese
%% Checkers. When it is working properly we will try with more players.
%% We will be using an iterative development model that emphasizes
%% getting the basics up and running quickly.

Our first computer player is an implementation of \emph{iterative
  deepening depth-first-search} (IDDFS) \cite{aimodern}. This has been
combined with heuristics to form a kind of informed search.

% We
% currently have two heuristics based on distance functions. The first
% is the sum of the Euclidean distance of all pegs from the goal. The
% second is a hand-tuned function that tries to penalize the bot when it
% is leaving pegs behind and attempts to keep pegs in the middle of the
% board, where it is more likely that good paths towards the goal will
% be formed. The heuristics described in \cite{ulfhake} will be very
% useful for our continued work.

\subsubsection{Heuristics}
Functions to evaluate boards must be used to estimate which moves are
better than others, and also to perform pruning. One idea we have used
is to consider the board as a two-dimensional array and compute the
Euclidean distance of all the pegs from their target position.

%%Currently both functions are implemented and can be used by the bots.
%% To penalize bots that leave pegs behind, it is possible to use the function
%% $d_{1}(x,y) = d(x,y)^{2}$ where \emph{d} is one of the already specified
%% distance functions. In this way the bots will reach better positions by making
%% the last peg to jump forward as far as possible.
%% TODO: the above is a bit speculative and should probably not be in the
%% final report.

Another board evaluation function that we have uses an hand-tuned
distance function that counts the the number of moves (excluding
jumps) necessary to reach the corner position in the target. The
hand-tuned distance function uses an array of weights: one for each
position on the board at which a peg can stop. The weights are then
summed up over all the player's pegs.

In different test games between the bots, it was noted that while moving to
non-central positions can constitute a disadvantage in two players games, it can
represent a good strategy on more populated boards where the moves available in
the central area are very limited. As a consequence the heuristics that prove to
be effective on one scenario can lead the bot to defeat in other scenarios.
Having said that, a good heuristic for two players games seems to assign more
value to boards that keep the pegs in the central area of the board, as
described in \cite{ulfhake}.

The approach of keeping all the pegs in a cluster to allow jumps and to give low
values to boards where the pegs of a certain player are scattered all around is
a good solution for two players games, but in other cases the board is crowded
so there are several opportunities to perform long jumps even for pegs isolated
from the other ones of the same color.

The three pegs located in the deepest positions of the corner risk to be trapped
by the opponent if they are left behind, so the hand-tuned distance function has
been modified to assign to these positions a great penalty.

But since we're in the field of Artificial Intelligence our thoughts
were drawn to the idea of having the computer make the distance
function itself. We have used our hand-tuned distance function to run
a genetic algorithm that finds better distance functions. More on this
in subsection \ref{genetic}.

\subsubsection{Startgame}
Until the players' pegs are close enough to interact with each other, an opening
book of pre-computed moves could be used. This would be useful just for two or
three players games since in all the other configurations players are located in
adjacent corners and can interact with each other already from the
 2\textsuperscript{nd} or 3\textsuperscript{rd} move.

\subsubsection{Endgame}
We have observed that in the phase of the endgame all the players' pegs are
located around their target corner, and there are no longer interactions between
them.
In this situation it is no longer necessary to use a \emph{minimax} algorithm,
because calculating the moves of the opponents is no longer necessary, so the
objective becomes to place all the marbles in the target using the least
possible amount of moves.
For the purpose the bots could switch to the \emph{iterative deepening
depth-first-search} or could even use a database of pre-computed moves.
In endgame phase a bot could consider a board where all the pegs of the
opponents are removed, and those boards could be hashed and used as keys for an
hash table that matches the best move for each board configuration.

%% To improve
%% the depth of our solution we will try and do some pruning of ``bad''
%% plys, e.g.~those that do not move the player any closer to the goal
%% state.

%% The literature describes algorithms that are more capable than
%% \emph{IDDFS}. An iterative development model means that we will be
%% making continual improvements to our program, and it is likely that we
%% will attempt to use some of the more capable algorithms later on. Our
%% first programs can be preserved for later programs to play against.

%% \subsubsection{Why do you think it will work?}% Not in the report template

%% Our solution will work because we think that \emph{IDDFS} closely
%% relates to how a human might play the game: first evaluating moves
%% that are easily visible on the board and iteratively looking deeper
%% (further) into what might happen.

%% Even though \cite{Huang:2001:SGP:378593.378708} does not report on the
%% performance of \emph{IDDFS} we know that it at least has been used to
%% some degree of success. The performance of our solution will of course
%% depend heavily on finding good heuristics. The first program might not
%% win any contests but it will be able to play the game, because our
%% heuristics can force it to make forward progress.

%% \subsection{Define an instance of the problem. How will you measure the
%%performance of your program?}% Not in the report template
%% %(You must know this in advance, find instances /benchmarks, or make your
%%own).

%% The program should play a game in the same way a human would play: the
%% program waits its turn and makes a move. The initial board setup is
%% one instance of the problem, but from the perspective of analysing the
%% game every intermediate state of the board may also be considered an
%% instance.

%% We want to evaluate the program's performance by having it play
%% against ourselves and other bots.


%% \subsection{The scope of your work}% Not in the report template
%% %(also, what interesting and related things are outside the scope of your
%%proposal?)

%% The scope of our project will ultimately be to try and challenge a set
%% of amateur human players. The first version of the program will play a
%% two-player game and this will later be extended to a multi-player
%% game. In developing the program we will explore how the dynamics of
%% the game changes in the multi-player version. Our program will be able
%% to play against itself, against humans, and against other programs. An
%% evaluation of the different versions of the program will be written.

%% Playing a perfect game must be outside our scope because it appears to
%% be an open research problem.

%% \subsection{Notes on strategies}% not in the report template

%% When we have the basic mechanics of the game and the computer player
%% working there are some strategies that we should consider. Some of the
%% strategies can be implemented by adjusting the heuristic function and
%% some can be implemented \emph{ah hoc}. The following are some ideas
%% for strategies.

%% \textbf{Leave no man behind}. If this strategy is not used it is
%% possible for the computer player to end up with a lot of pegs very far
%% away from the goal. These pegs will be very costly to move toward the
%% goal if it's late in the game, so a heuristic should take that into
%% account.

%% \textbf{Stay the path}. The center of the board is more likely to
%% contain many paths towards the goal. Therefore if the heuristic gives
%% a preference to moves that keep pegs in the middle then the game is
%% likely to move faster and give more opportunities to jump long
%% distances.

%% \textbf{Forge ahead} or \textbf{the best offense is a good defense}.
%% The computer player can either be aggressive and try to create long
%% paths for moving fast and far, or it can be defensive and try to not
%% give any other players opportunities to use its pegs as part of a
%% path.

\section{Overview of the architecture}
% (Describe the different parts of your program suite in detail.)
\subsection{Running modules}
%% arena  cclient   gui_client
%% bot    chinese   hs-cli

Our project is divided into a collection of programs that each solve
different parts of the problem. The programs communicate using a
network protocol that we have designed. Our choice to use a network
protocol has let us write the programs in different programming
languages. We have one server implementation, a graphical client and a
bot client.

%% TODO: note the cli client and the hs client?

Players connect to a \emph{game server} which maintains the current
game state, notifies each player of their turn and checks that the
rules of the game are not violated. A player can host a game or join a
game someone else is hosting. It is also possible to join as a
spectator and see what is going on in a game. If the game is over the
server also announces who the winner was. The server is written in
Erlang and can host multiple games simultaneously.

The \emph{graphical client} is written in Python and is designed for a
human player. The main part of the interface is the current board
state, shown as the star that one would normally see a on real life
version of the game. When it is the players' turn they can make their
move by clicking on the pegs shown on the screen.

The \emph{bot} is the program that contains the AI components of our
project. It is also written in Python and is divided in three major
components: the protocol module, the board module and the bot module.
The \emph{protocol module} serializes and deserializes the network protocol.
The \emph{board module} contains code to generate all moves a player has
available and it also contains our board evaluation functions. The
\emph{bot module} contains the search algorithms. It is the job of the search
algorithms to use the possible moves and the board evaluation
functions to determine which move will give the best outcome for the
bot. When we have implemented a new search algorithm or board
evaluation function the previous ones are still available. Each
combination of search algorithm and evaluation function is given a
\emph{bot personality} identifier.

An important part of our project is not only to implement search
algorithms and heuristics, but to also measure their performance. To
aid in this we have the \emph{arena} program. This program can start
up a game between different bot personalities so that we can determine
which bot is the strongest in different situations.

%%\subsection{Modules designed but not implemented}

%%\subsection{Modules a future continuation may have}

\section{Results and Evaluation}
% (What does your running code do? How does it fare against your
% benchmarks and instances? Describe advantages and disadvantages,
% possibly in relation to other groups in this course.)

To be able to test different approaches, the bot supports multiple
functions. In general, Python is a slow interpreted language, but
there are projects that aim at providing just in time compilation. The
PyPy interpreter has proven to be somewhat faster than the standard
CPython. When PyPy is forced to compile loops, after a few iterations,
the IDDFS algorithm is able to compute one level deeper. Because the
algorithm is time-limited this gives the bot running on PyPy an
advantage over the bots running on CPython.

We have a time-limited implementation of iterative-deepening depth
first search based on an algorithm from \cite{aimodern}. When the
algorithm compares two moves that achieve an equally good outcome it
will explicitly favor the move that achieves the outcome sooner (at a
lower search depth). If this check is omitted the algorithm can end up
looping at the end of a game. This is because it would make a move
that would be the winning move two moves into the future, but on the
next turn it might again choose a move that wins two moves into the
future, \emph{ad infinitum}.

\begin{figure}
\begin{algorithmic}
\Function{IDDFS}{board, player-id, time-limit}
\State best.move = \emph{nil}
\State best.score = $-\infty$
\State best.depth = $\infty$
\For{depth $= 0 \to \infty$}
 \ForAll{move $\in$ \textsc{All-Possible-Moves}(board, player-id)}
  \State nboard = \textsc{Update-Board}(board, move)
  \State \textsc{Recursive-DLS}(move, nboard, depth, 0, best)
 \EndFor
 \If{timeout}
  \State \Return best.move
 \EndIf
\EndFor
\EndFunction
\\
\Function{Recursive-DLS}{move1, board, limit, depth, best}
\State score = \textsc{Board-Evaluation}(board, player-id)
\If{score $>$ best.score $\vee$ (score $=$ best.score $\wedge$ depth $<$
best.depth)}
\State best.move = move1
\State best.score = score
\State best.depth = depth
\EndIf
\If{limit $\le$ 0 $\vee$ timeout}
\State \Return
\EndIf
\ForAll{move $\in$ \textsc{All-Possible-Moves}(board, player-id)}
 \State nboard = \textsc{Update-Board}(board, move)
 \State \textsc{Recursive-DLS}(move1, nboard, limit$-1$, depth$+1$, best)
\EndFor
\EndFunction
\end{algorithmic}
\caption{Iteratively-deepening depth-limited search with a time limit.}
\end{figure}

The previous \emph{IDDFS-algorithm} was a very naive algorithm because it only
considers its own moves and never the opponent's. Because of this the bots
agenda can be destroyed by the opponent by blocking the path it was about
to take.

To counter this we can use the \emph{MINIMAX-algorithm}. The
\emph{MINIMAX-algorithm} tries to \emph{minimize} the board-evaluation function
for itself while trying to maximize the board-evaluation function of its
opponent.

%% MINIMAX
\begin{figure}
\begin{algorithmic}
\Function{Minimax}{board, player-id, depth}
  \State best.score = $-\infty$
  \ForAll{move $\in$ \textsc{All-Possible-Moves}(board, player-id)}
    \State val = -\textsc{Minimax-Value}(\textsc{Update-Board}(board, move), \\
    \hspace{160pt} player-id, depth-1)
    \If{val $>$ score}
      \State best.score = val
      \State best.move = move
    \EndIf
  \EndFor
  \State \Return best.move
\EndFunction
\\
\Function{Minimax-Value}{board, player-id, depth}
\If{\textsc{Won}(board, player-id)}
  \State \Return winning score
\ElsIf{depth $=$ 0}
  \State \Return \textsc{Board-Evaluation}(board, player-id)
\Else
  \State opp-id = \textsc{Opponent}(player-id)
  \State best = $-\infty$
  \ForAll{move $\in$ \textsc{Possible-Moves}(board, opp-id)}
    \State val = -\textsc{Minimax-Value}(\textsc{Update-Board}(board, move), \\
    \hspace{180pt} opp-id, depth-1)
    \If{val $>$ best}
      \State best = val
    \EndIf
  \EndFor
  \State \Return best
\EndIf
\EndFunction
\end{algorithmic}
\caption{Minimax algorithm.}
\end{figure}

%% ALPHABETA
%% Reasons for alpha-beta over naive minimax.
%% -- We need to prune because of the huge search tree
%%    to gain more depth in the search.
%% pruning tactics?

%% Fill in with ideas for what to write about:

%% * The multiplayer aspect
%% ** The search-tree explodes with many players
%%    -- Especially in the middle of the game when alot of more paths open up
%% ** So what else changes with three or more players?
%%    -- Connected to the last one, there may be more paths around the
%%       middle than through it with more players
%% ** What about a bot that recognizes others of its ilk and conspires?
%% * 2/3-phase bot? Using IDDFS-bot in the beginning, reverting to minimax
%%   and maybe ending with iddfs also?
%% * How does the bot play against a human?

\subsection{Genetic algorithm}
\label{genetic}

It is difficult to know \emph{a priori} what a good board evaluation
function\footnote{A board evaluation function measures how good a
  board appears to be for the player. The score is higher when pegs
  are closer to the goal.} looks like. All our heuristics are encoded
in the board evaluation function, so it is vital for the performance
of our bot. Our hand-tuned function is shown in figure
\ref{handtuned}. It is meant to prefer moving pegs into positions
incrementally closer to the goal. There is a preference for positions
in the middle path of the board and a distinct penalty for leaving
pegs in the ``point'' of the starting position.

\begin{figure}
\centering
\include{dist-array-v4}
\caption{Our hand-tuned distance function. Pegs placed in the brighter
  positions give a lower score. Our bots use a search algorithm to
  find moves that place all pegs in the darkest positions, i.e.~it
  finds the moves that maximize the board evaluation function.}
\label{handtuned}
\end{figure}

The problem is that it's difficult to get a feel for how this function
acts when summed up over all pegs. Consider the case when the three
pegs at the top of the star are still in their starting position and
the seven other pegs are closer to the goal. There are two clusters of
pegs and if the board evaluation function has not been properly
designed then only the cluster closer to the goal will keep moving.
The cluster still in the starting position will only start moving
later in the game, when there are less opportunities for jumps.

The genetic algorithm that we used is shown in figure \ref{genalg}. It
never returns, but in our program it prints the most fit of each
generation. The algorithm is called with a population of (almost)
completely random individuals. To speed up the process the initial
individuals are not random in the start and goal positions. Figure
\ref{genpop0} shows a first-generation individual. The algorithm
measures the fitness of all individuals in the population and only the
most fit $20\%$ is kept. These individuals then mix to create a new
population with as many individuals as in the original population.
There is a $6.67\%$ chance that one of the new individuals will have
mutations. The mutations are guided so that it is less likely that
they will be detrimental.

The algorithm is not foolproof. Sometimes the surviving populations
are hopeless--they never start to have positive fitness values. If
that happens a new random population needs to be created.

\begin{figure}
\begin{algorithmic}
\Function{Genetic-Algorithm}{population, fitness-fn}
\While{$\top$}
 \State survivors = \textsc{Most-Fit}(population, fitness-fn, $1/5$)
 \State new-population = \emph{nil}
 \For{$\textrm{i} = 0 \to |\textrm{survivors}|$}
   \State x = \textsc{Random-Select}(survivors)
   \State y = \textsc{Random-Select}(survivors)
   \State child = \textsc{Reproduce}(x, y)
   \If{$\textsc{Random-Integer}(15)=0$}
     \State \textsc{Mutate}(child)
   \EndIf
   \State new-population[i] = child
 \EndFor
\EndWhile
\EndFunction
\\
\Function{Reproduce}{x, y}
\State z = \emph{nil}
\For{$\textrm{i} = 0 \to |\textrm{x}|$}
 \If{$\textsc{Random-Integer}(2) = 0$}
  \State z[i] = x[i]
 \Else
  \State z[i] = y[i]
 \EndIf
\EndFor
\State \Return z
\EndFunction
\\
\Function{Mutate}{x}
\State goal-positions = \emph{The positions in the player's goal}
\For{$\textrm{i} = 0 \to |\textrm{x}|$}
 \If{$\textsc{Random-Integer}(10) = 0$}
  \State $\textrm{v} = \textsc{Random-Integer}(50) - 25$
  \If{$\textrm{v} > 0 \vee \textrm{i} \in \textrm{goal-positions}$}
    \State x[i] = v
  \EndIf
 \EndIf
\EndFor
\ForAll{$\textrm{i} \in \textrm{goal-positions}$}
 \State x[i] = $\textsc{Min}(\textrm{x}[\textrm{i}], 5)$
\EndFor
\EndFunction
\end{algorithmic}

\caption{The genetic algorithm we used to derive a better board
  evaluation function. It is losely based on an algorithm from
  \cite{aimodern}.}
\label{genalg}
\end{figure}

The fitness function is the key to this genetic algorithm. Figure
\ref{fitness} shows that the early generations are not very successful
at all. Then after a while some individuals start winning games and
the general fitness of their offspring increases dramatically. After
the first thousand generations the fitness only increases marginally.
Figure \ref{winning} shows the rate at which the populations win. Our
fitness function is shown in figure \ref{fitnessfn}.

The \textsc{Simulation} function runs a game between two bots. In our
case it runs the generated bot against our hand-tuned bot. Both bots
are using a trivial search function that merely sorts all available
moves by the board evaluation function and does no lookahead. If we
used a more advanced search algorithm it would take much longer to run
the genetic algorithm, and we do not have the computing resources
required for that. Individuals from the early generations are likely
going to leave some pegs behind in the starting position, which
prevents either player from winning. In this case the simulation is
restarted.

The fitness function runs two simulations per individual and averages
the score. This gives an adequate and fast estimate for the early
generations. Later when the fitness function detects that the
individual is good at winning (it has an aggregate score over $300$)
it will switch over to running five simulations. This gives more
accurate estimates of an individual's fitness and decreases the
likelihood that a good individual will be penalized for what can be
considered as bad luck.

If the individual wins a simulation it is awarded $500-p$ points,
where $p$ is how many plies it needed to win. If it loses it is
penalized with $d$ points, where $d$ is a measure of how far away it
was from winning. This is computed using the same board evaluation
function that drives the bot used in the simulation. Thusly when the
bot populations are losing, they will try to be as close to winning as
possible. Later when they are winning they will be trying to win with
as few plies as possible.

\begin{figure}
\begin{algorithmic}
\Function{Fitness}{individual}
\State score = 0
\State iteration = 0
\While{$\top$}
 \If{$\textrm{iterations} = 2 \wedge \textrm{score} < 300$}
   \State \Return $\textrm{score} / 2$
 \ElsIf{$\textrm{iterations} = 5$}
   \State \Return $\textrm{score} / 5$
 \EndIf
 \State $\textrm{winner}, \textrm{plies}, \textrm{distance} = \textsc{Simulation}(\textrm{individual})$
 \If{$\textrm{winner} = \textrm{individual}$}
   \State $\textrm{score} = \textrm{score} + (500 - \textrm{plies})$
 \Else
   \State $\textrm{score} = \textrm{score} - \textrm{distance}$
 \EndIf
\EndWhile
\EndFunction
\end{algorithmic}
\caption{The fitness function computes a fitness score for an
  individual by having it play against a pre-defined bot.}
\label{fitnessfn}
\end{figure}


\begin{figure}
\centering
\includegraphics{population-fitness.pdf}
\caption{The fitness of the populations generated by the genetic
  algorithm improves over time. A lost game gives a negative fitness
  which gets closer to zero the closer the individual was to winning.
  Positive fitness means the game was won. The fewer moves were
  used, the higher the fitness becomes.}
\label{fitness}
\end{figure}

\begin{figure}
\centering
\includegraphics{population-winning-percentage.pdf}
\caption{The percentage of games won by the populations generated by
  the genetic algorithm.}
\label{winning}
\end{figure}

Figures \ref{genpop0}, \ref{genpop51}, \ref{genpop2427} and
\ref{genpop7822} show distance functions from different generations.
The first and second functions are not very useful in themselves, but
provide a point of comparison. It is interesting to note that in the
latter generations the genetic algorithm has built a path of darker
positions leading up the goal. Another noteworthy point is the darker
position in the lower right corner. There does not appear to be any
benefit to having a peg in this position. This is most likely an
artifact caused by the fitness function we used: in most two-player
games it is unlikely that there will be a peg in this position, so
there has been no evolutionary pressure to make this position lighter.
This will be a drawback in games with more than two players.

The genetic algorithm is easily adapted to simulate games between
three players. Figure \ref{gen3pop3594} shows an individual from such
a simulation. It is remarkably similar to the two-player individual
from figure \ref{genpop7822}. A three-player game exercises more of
the positions on the board and therefore places greater demands on the
individual. The dark positions are no longer isolated from the rest of
the board: the are connected to the goal by some path. Running
simulations with three players not only requires more CPU time, but
also requires more generations before it gets good results (figures
\ref{fitness} and \ref{winning}).

\begin{figure}
\centering
\include{genetic-population-0}
\caption{The most fit distance function at generation 0. The first
  generation is fully random except for the start and goal positions.
  This individual did not win any games at all.}
\label{genpop0}
\end{figure}

\begin{figure}
\centering
\include{genetic-population-51}
\caption{This is the first individual that has actually won a game
  against our hand-tuned bot. It is from generation 51.}
\label{genpop51}
\end{figure}

\begin{figure}
\centering
\include{genetic-population-2427}
\caption{The individuals from generation 2427 won every game they
  played when the genetic algorithm was running. Each individual is
  only tested five times by the algorithm, but more extensive
  experiments show that this individual wins around $99\%$ of the
  games it plays. The fitness of this individual was $416.6$.}
\label{genpop2427}
\end{figure}

\begin{figure}
\centering
\include{genetic-population-7822}
\caption{This individual from generation 7822 had a fitness of
  $427.4$, the highest of all individuals generated. When tested by
  the genetic algorithm it won in an average of only $500-427.4=72.6$
  moves.}
\label{genpop7822}
\end{figure}

\begin{figure}
\centering
\include{genetic-3p-superior-population-3594}
\caption{We extended the algorithm to simulate three-player games.
  This individual comes from population 3594 in such a run. It had
  $99.2\%$ wins and a fitness of $393.4$.}
\label{gen3pop3594}
\end{figure}

\section{Discussion and Conclusions}
%(Sum up your project, suggest future extensions and improvements.)


% \section{References}
% (Re-use from previous documents, extend.)
\bibliographystyle{aiaa}
\bibliography{refs}

\section{Appendix}
% (Here you include the diary and all other information and
% documentation that did not fit into the report in the above sections
% but that you consider too important to leave out.)

%% A quick tutorial on how the server works

%% Maybe a quick overlook of the gui-client?

%% Maybe a copy of the numbers for the distance arrays that were
%% presented graphically.

\end{document}
